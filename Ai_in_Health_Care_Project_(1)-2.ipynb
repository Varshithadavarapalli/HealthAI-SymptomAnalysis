{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPRaBnkolz0b",
        "outputId": "82236f52-dce1-4ca4-b005-dad43158badd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "    Unnamed: 0      label                                               text\n",
            "0           0  Psoriasis  I have been experiencing a skin rash on my arm...\n",
            "1           1  Psoriasis  My skin has been peeling, especially on my kne...\n",
            "2           2  Psoriasis  I have been experiencing joint pain in my fing...\n",
            "3           3  Psoriasis  There is a silver like dusting on my skin, esp...\n",
            "4           4  Psoriasis  My nails have small dents or pits in them, and...\n",
            "\n",
            "Column Names in Dataset: Index(['Unnamed: 0', 'label', 'text'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Dataset\n",
        "dataset_path = \"/content/Symptom2Disease.csv\"  # Ensure the correct file path\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Dataset Preview:\\n\", df.head())\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nColumn Names in Dataset:\", df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5dWXFGa8sm3U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBh7jRW8spnT",
        "outputId": "d8cf890a-1ea9-45b3-f4a5-d1bebe8be0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'stopwords' resource...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Available Columns in Dataset: ['unnamed: 0', 'label', 'text']\n",
            "\n",
            " Detected Columns: Symptoms -> 'text', Disease -> 'label'\n",
            "\n",
            " Preprocessing Completed!\n",
            "   unnamed: 0      label                                               text  \\\n",
            "0           0  Psoriasis  I have been experiencing a skin rash on my arm...   \n",
            "1           1  Psoriasis  My skin has been peeling, especially on my kne...   \n",
            "2           2  Psoriasis  I have been experiencing joint pain in my fing...   \n",
            "3           3  Psoriasis  There is a silver like dusting on my skin, esp...   \n",
            "4           4  Psoriasis  My nails have small dents or pits in them, and...   \n",
            "\n",
            "                                    cleaned_symptoms  \n",
            "0  experiencing skin rash arms legs torso past we...  \n",
            "1  skin peeling especially knees elbows scalp pee...  \n",
            "2  experiencing joint pain fingers wrists knees p...  \n",
            "3  silver like dusting skin especially lower back...  \n",
            "4  nails small dents pits often feel inflammatory...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Download stopwords if not already available\n",
        "try:\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "except LookupError:\n",
        "    print(\"Downloading 'stopwords' resource...\")\n",
        "    nltk.download(\"stopwords\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Load Dataset\n",
        "dataset_path = \"/content/Symptom2Disease.csv\"  # Ensure the correct file path\n",
        "try:\n",
        "    df = pd.read_csv(dataset_path)\n",
        "except FileNotFoundError:\n",
        "    print(\" File not found. Please check the dataset path.\")\n",
        "    raise\n",
        "\n",
        "# Standardize column names (Remove spaces, convert to lowercase)\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "# Print available columns\n",
        "print(\"\\n Available Columns in Dataset:\", df.columns.tolist())\n",
        "\n",
        "# Auto-detect symptom and disease columns\n",
        "possible_symptom_cols = [\"symptom\", \"text\", \"description\"]\n",
        "possible_disease_cols = [\"disease\", \"label\", \"condition\"]\n",
        "\n",
        "# Identify columns dynamically\n",
        "symptom_col = next((col for col in df.columns if any(key in col for key in possible_symptom_cols)), None)\n",
        "disease_col = next((col for col in df.columns if any(key in col for key in possible_disease_cols)), None)\n",
        "\n",
        "# Assign default names if not detected\n",
        "if not symptom_col or not disease_col:\n",
        "    print(\" Column names not recognized. Using default: 'text' as symptoms, 'label' as disease.\")\n",
        "    symptom_col = \"text\" if \"text\" in df.columns else df.columns[0]\n",
        "    disease_col = \"label\" if \"label\" in df.columns else df.columns[1]\n",
        "\n",
        "print(f\"\\n Detected Columns: Symptoms -> '{symptom_col}', Disease -> '{disease_col}'\")\n",
        "\n",
        "# Text Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"\\W\", \" \", text)  # Remove special characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "    words = text.split()  # Faster than word_tokenize()\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply Preprocessing\n",
        "df[\"cleaned_symptoms\"] = df[symptom_col].astype(str).apply(preprocess_text)\n",
        "\n",
        "print(\"\\n Preprocessing Completed!\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zxkP9HgMsq_T"
      },
      "outputs": [],
      "source": [
        "# Convert text into numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"cleaned_symptoms\"])\n",
        "y = df[disease_col]\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkzVcyzTsvBD",
        "outputId": "ee80e261-2970-4be9-cb7e-a0afc9b5f258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model Performance Comparison:\n",
            "\n",
            "🔹 Naïve Bayes:\n",
            "Accuracy: 0.9542\n",
            "Precision: 0.9617\n",
            "Recall: 0.9542\n",
            "F1 Score: 0.9506\n",
            "\n",
            "🔹 Logistic Regression:\n",
            "Accuracy: 0.9792\n",
            "Precision: 0.9812\n",
            "Recall: 0.9792\n",
            "F1 Score: 0.9790\n",
            "\n",
            "🔹 SVM (Support Vector Machine):\n",
            "Accuracy: 0.9792\n",
            "Precision: 0.9812\n",
            "Recall: 0.9792\n",
            "F1 Score: 0.9792\n",
            "\n",
            "🔹 Random Forest:\n",
            "Accuracy: 0.9750\n",
            "Precision: 0.9779\n",
            "Recall: 0.9750\n",
            "F1 Score: 0.9741\n",
            "\n",
            "🔹 Gradient Boosting:\n",
            "Accuracy: 0.8375\n",
            "Precision: 0.8949\n",
            "Recall: 0.8375\n",
            "F1 Score: 0.8536\n"
          ]
        }
      ],
      "source": [
        "# Define multiple ML models\n",
        "models = {\n",
        "    \"Naïve Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
        "    \"SVM (Support Vector Machine)\": SVC(kernel=\"linear\"),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"Recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "# Display Results\n",
        "print(\"\\n Model Performance Comparison:\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"\\n🔹 {model}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CFywcwaxtYvs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjsD0U5VtfoD",
        "outputId": "f892d392-60b6-4d40-8843-e6b671a579c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            " Best Random Forest Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            " Best Gradient Boosting Parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}\n",
            "\n",
            " Optimized Random Forest Accuracy: 0.9750\n",
            " Optimized Gradient Boosting Accuracy: 0.8417\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Define hyperparameter grid for Gradient Boosting\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 10]\n",
        "}\n",
        "\n",
        "# Run GridSearchCV for Random Forest\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "# Run GridSearchCV for Gradient Boosting\n",
        "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(f\" Best Random Forest Parameters: {rf_grid.best_params_}\")\n",
        "print(f\" Best Gradient Boosting Parameters: {gb_grid.best_params_}\")\n",
        "\n",
        "# Evaluate the best models\n",
        "rf_best = rf_grid.best_estimator_\n",
        "gb_best = gb_grid.best_estimator_\n",
        "\n",
        "rf_pred = rf_best.predict(X_test)\n",
        "gb_pred = gb_best.predict(X_test)\n",
        "\n",
        "print(f\"\\n Optimized Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
        "print(f\" Optimized Gradient Boosting Accuracy: {accuracy_score(y_test, gb_pred):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rrUpQ4CYzM_F"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MU2_LiZyzQxc"
      },
      "outputs": [],
      "source": [
        "# Convert labels to numbers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_dl = label_encoder.fit_transform(y)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df[\"cleaned_symptoms\"])\n",
        "X_dl = tokenizer.texts_to_sequences(df[\"cleaned_symptoms\"])\n",
        "\n",
        "# Pad sequences to ensure same length\n",
        "X_dl = pad_sequences(X_dl, maxlen=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2iEHGBmzS58",
        "outputId": "0238f242-e95b-45cc-9c92-3b44eeed3777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 180ms/step - accuracy: 0.1398 - loss: 3.1377 - val_accuracy: 0.0000e+00 - val_loss: 4.3316\n",
            "Epoch 2/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.2087 - loss: 2.7959 - val_accuracy: 0.0000e+00 - val_loss: 5.4126\n",
            "Epoch 3/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 0.5030 - loss: 1.9868 - val_accuracy: 0.0000e+00 - val_loss: 6.4614\n",
            "Epoch 4/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.7633 - loss: 1.1690 - val_accuracy: 0.0000e+00 - val_loss: 7.9131\n",
            "Epoch 5/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - accuracy: 0.8640 - loss: 0.6553 - val_accuracy: 0.0000e+00 - val_loss: 7.7595\n",
            "Epoch 6/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 140ms/step - accuracy: 0.9203 - loss: 0.4309 - val_accuracy: 0.0000e+00 - val_loss: 8.1576\n",
            "Epoch 7/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.9386 - loss: 0.3017 - val_accuracy: 0.0000e+00 - val_loss: 8.3556\n",
            "Epoch 8/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9582 - loss: 0.1917 - val_accuracy: 0.0167 - val_loss: 8.4556\n",
            "Epoch 9/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.9661 - loss: 0.1417 - val_accuracy: 0.0375 - val_loss: 8.0168\n",
            "Epoch 10/10\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - accuracy: 0.9832 - loss: 0.1125 - val_accuracy: 0.0250 - val_loss: 7.4567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e8a29612c10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=50),\n",
        "    SpatialDropout1D(0.2),\n",
        "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_dl, y_dl, epochs=10, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FCPjuABzi31",
        "outputId": "8839ae51-ca92-4139-82d5-5be11b75c5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install flask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-veR2APEzmp7",
        "outputId": "c674d24f-5925-4453-e499-877b2b6fec55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model and Vectorizer saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#  Load Dataset\n",
        "dataset_path = \"/content/Symptom2Disease.csv\"  # Ensure correct file path\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "#  Standardize column names\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "#  Detect relevant columns\n",
        "symptom_col = \"text\" if \"text\" in df.columns else df.columns[1]\n",
        "disease_col = \"label\" if \"label\" in df.columns else df.columns[2]\n",
        "\n",
        "#  Preprocessing function\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"\\W\", \" \", text)  # Remove special characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "    words = text.split()  # Uses split instead of word_tokenize()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "#  Apply Preprocessing\n",
        "df[\"cleaned_symptoms\"] = df[symptom_col].astype(str).apply(preprocess_text)\n",
        "\n",
        "#  TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"cleaned_symptoms\"])\n",
        "y = df[disease_col]\n",
        "\n",
        "#  Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Train Model (Using Naïve Bayes)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Save Model and Vectorizer\n",
        "with open(\"model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "with open(\"vectorizer.pkl\", \"wb\") as vectorizer_file:\n",
        "    pickle.dump(vectorizer, vectorizer_file)\n",
        "\n",
        "print(\" Model and Vectorizer saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok streamlit pandas scikit-learn nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU63lqfu3QsR",
        "outputId": "ae02dc00-90c8-4ecf-8d65-58708fb98500"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.33.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, flask-ngrok, streamlit\n",
            "Successfully installed flask-ngrok-0.0.25 pydeck-0.9.1 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#  Ensure NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "#  Load Dataset\n",
        "df = pd.read_csv(\"Symptom2Disease.csv\")\n",
        "\n",
        "#  Detect Columns\n",
        "symptom_col = \"text\"\n",
        "disease_col = \"label\"\n",
        "\n",
        "#  Preprocessing Function\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"\\W\", \" \", str(text).lower())  # Remove special characters\n",
        "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "#  Apply Preprocessing\n",
        "df[\"cleaned_symptoms\"] = df[symptom_col].apply(preprocess_text)\n",
        "\n",
        "#  Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df[\"cleaned_symptoms\"])\n",
        "y = df[disease_col]\n",
        "\n",
        "#  Train Model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Save Model & Vectorizer\n",
        "pickle.dump(model, open(\"model.pkl\", \"wb\"))\n",
        "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "print(\" Model and Vectorizer saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDRHiUuI7Qnj",
        "outputId": "f206faf2-ee6b-499e-e5dc-543e5a8c8370"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model and Vectorizer saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "#  Load Model and Vectorizer\n",
        "model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
        "vectorizer = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n",
        "\n",
        "#  Function to Predict Disease\n",
        "def predict_disease(symptoms):\n",
        "    processed_text = \" \".join(symptoms.lower().split())  # Simple cleaning\n",
        "    vectorized_text = vectorizer.transform([processed_text])\n",
        "    return model.predict(vectorized_text)[0]\n",
        "\n",
        "#  Example Usage\n",
        "symptoms = input(\"Enter your symptoms (comma-separated): \")\n",
        "predicted_disease = predict_disease(symptoms)\n",
        "print(f\" Predicted Disease: {predicted_disease}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gb2f5rc7qW5",
        "outputId": "1c9b12d0-64db-472e-b949-263eb19a4653"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your symptoms (comma-separated): Joint pain\n",
            " Predicted Disease: Dengue\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}